{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwVF0O3ezf7F",
        "outputId": "f9667ab1-a31e-47f5-ea2c-22c9d411334c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def print_past_key_values(past_key_values):\n",
        "    # links that roughly explains past_key_values:\n",
        "    #  https://huggingface.co/docs/transformers/model_doc/t5\n",
        "    # Better definitions:\n",
        "    # The length of past_key_values is num_hidden_layers.\n",
        "    # For each hidden layer, there are two elements. Each element is a tuple,\n",
        "    #  representing either key or values.\n",
        "    # Each key and value is a tensor of (batch_size, num_heads, sequence_length, head_size)ã€‚\n",
        "\n",
        "    print(\"type of past_key_values: \", type(past_key_values))\n",
        "    print(\"type of past_key_values[0]: \", type(past_key_values[0]))\n",
        "    print(\"type of past_key_values[0][0]: \", type(past_key_values[0][0]))\n",
        "    print(\"past_key_values shape: (\",\n",
        "          len(past_key_values), \", \",\n",
        "          len(past_key_values[0]), \", \",\n",
        "          past_key_values[0][0].shape, \")\")\n",
        "    print(\"More details on past_key_values shape: (\",\n",
        "          len(past_key_values), \"-num_hidden_layers, \",\n",
        "          len(past_key_values[0]), \"-key/value, \",\n",
        "          len(past_key_values[0][0]), \"-batch_size, \",\n",
        "          len(past_key_values[0][0][0]), \"-num_heads, \",\n",
        "          len(past_key_values[0][0][0][0]), \"-sequence_length, \",\n",
        "          len(past_key_values[0][0][0][0][0]), \"-head_size)\")\n",
        "\n",
        "\n",
        "def generate_on_complete_context(model, tokenizer):\n",
        "    print (\"---------------generate_on_complete_context----------------------\")\n",
        "    # Encode the initial text\n",
        "    ids = tokenizer.encode(\"Hello, my dog is big\", return_tensors=\"pt\")\n",
        "\n",
        "    print(\"ids: \", ids)\n",
        "\n",
        "    # Generate a continuation with the complete context\n",
        "    output = model.generate(\n",
        "        ids, max_new_tokens=20, num_return_sequences=1\n",
        "    )\n",
        "    print(\"output[0]: \", output[0])\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(\"Generated text with complete context:\", generated_text)\n",
        "\n",
        "def genererate_using_past_key_values(model, tokenizer):\n",
        "    print (\"---------------genererate_using_past_key_values----------------------\")\n",
        "\n",
        "    # Encode the initial text\n",
        "    ids = tokenizer.encode(\"Hello, my dog is big\", return_tensors=\"pt\")\n",
        "    print(\"ids: \", ids)\n",
        "    print(\"ids length: \", len(ids))\n",
        "\n",
        "    # Split the input into two parts: \"Hello, my dog is\" and \"big\"\n",
        "    context_ids = ids[:, :-1]\n",
        "    next_word_ids = ids[:, -1:]\n",
        "    print(\"context_ids: \", context_ids)\n",
        "    print(\"next_word_ids: \", next_word_ids)\n",
        "\n",
        "    # Generate a continuation using caching (past_key_values)\n",
        "    output = model.generate(input_ids=context_ids, max_new_tokens = 1, use_cache=True, return_dict_in_generate=True)\n",
        "    print(\"output[0]: \", output[0])\n",
        "    past_key_values = output.past_key_values\n",
        "    print_past_key_values(past_key_values)\n",
        "\n",
        "    # Generate the next word based on the cached context\n",
        "    # output = model(input_ids=next_word_ids, past_key_values=past_key_values)\n",
        "    output = model.generate(input_ids=ids,  max_new_tokens = 10, past_key_values=past_key_values, return_dict_in_generate=True)\n",
        "    print(output['sequences'][0])\n",
        "    generated_text = tokenizer.decode(output['sequences'][0], skip_special_tokens=True)\n",
        "    print(\"Generated text with complete context:\", generated_text)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "generate_on_complete_context(model, tokenizer)\n",
        "genererate_using_past_key_values(model, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mDdgyDezkua",
        "outputId": "4ca1e5a5-ba7b-446e-e4aa-cb2f956dad93"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------generate_on_complete_context----------------------\n",
            "ids:  tensor([[15496,    11,   616,  3290,   318,  1263]])\n",
            "output[0]:  tensor([15496,    11,   616,  3290,   318,  1263,   290,   314,  1101,   407,\n",
            "         1654,   611,   339,   338,  1016,   284,   307,  1498,   284,  2513,\n",
            "           13,   314,  1101,  1016,   284,   423])\n",
            "Generated text with complete context: Hello, my dog is big and I'm not sure if he's going to be able to walk. I'm going to have\n",
            "---------------genererate_using_past_key_values----------------------\n",
            "ids:  tensor([[15496,    11,   616,  3290,   318,  1263]])\n",
            "ids length:  1\n",
            "context_ids:  tensor([[15496,    11,   616,  3290,   318]])\n",
            "next_word_ids:  tensor([[1263]])\n",
            "output[0]:  tensor([[15496,    11,   616,  3290,   318,   257]])\n",
            "type of past_key_values:  <class 'tuple'>\n",
            "type of past_key_values[0]:  <class 'tuple'>\n",
            "type of past_key_values[0][0]:  <class 'torch.Tensor'>\n",
            "past_key_values shape: ( 12 ,  2 ,  torch.Size([1, 12, 5, 64]) )\n",
            "More details on past_key_values shape: ( 12 -num_hidden_layers,  2 -key/value,  1 -batch_size,  12 -num_heads,  5 -sequence_length,  64 -head_size)\n",
            "tensor([15496,    11,   616,  3290,   318,  1263,   290,   314,  1101,   407,\n",
            "         1654,   611,   339,   338,  1016,   284])\n",
            "Generated text with complete context: Hello, my dog is big and I'm not sure if he's going to\n"
          ]
        }
      ]
    }
  ]
}